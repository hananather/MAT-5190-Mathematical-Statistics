\section{November 24, 2021}
\subsection{Equivariance Principle}
\textbf{Equivariance Principle:} If $\boldsymbol{Y} = g(\boldsymbol{X})$ is a change of measurement scale such that the model for $\boldsymbol{Y}$ has the same formal structure as the model for $\boldsymbol{X}$,  then inference procedure should be both measurement equivariant and formally equivariant. 
\begin{definition}[Group transformation]
    A set $\SG$ of functions, $\{ g(\boldsymbol{x}: g \in \SG) \}$, of the form $g: \SX \to \SX$ is called a \vocab{group transformation} of $\SX$ if
    \begin{enumerate}[(i)]
        \item (Inverse) $\forall g \in \SG$, $\exists g^{-1} \in \SG$ such that $g\circ g^{-1} = e$, where $e: \SX \to \SX$ is the identity $e(x) = x$. 
        \item (Composition) For every $g,g^{'} \in \SG$, $g \circ g^{'} \in \SG$
        \item (Identity) The identity, $e(\boldsymbol{x})$ is an element of $\SG$.
    \end{enumerate}
\end{definition}
\begin{definition}[Invariant]
    Let $\SF = \{f(\boldsymbol{x}|\theta): \theta \in \Theta \}$ be a set of pdf or pmfs for $\boldsymbol{X}$, and let $\SG$ be a group of transformations of the  sample space $\SX$. Then $\SF$ is \vocab{invariant under the group $G$} if for every $\theta \in \Theta$ and $g \in \SG$ there exists a unique $\theta^{'} \in \Theta$ such that $\boldsymbol{Y} = g(\boldsymbol{X})$ has the distribution $f(\boldsymbol{y}|\theta^{'})$ if $\boldsymbol{X}$ has the distribution $f(\boldsymbol{x}|\theta)$.
\end{definition}
The equivariance principle essentially says that inference based on $\boldsymbol{X}$ should be the same as inference based on $\boldsymbol{Y}$.
\begin{example}[Location family is invariant]

\end{example}
\begin{example}[Scale family is invariant]

\end{example}
\subsection{Methods for finding estimators}
A statistic is a function of the random sample, and we generally use such functions to approximate the value of the unknown parameter $\theta$, which is underlying the parameter for the underlying distribution of the sample) called \vocab{estimator}. Once we have observed a particular realization of $\boldsymbol{X}$, (the sample $\boldsymbol{x}$), we refer to the value $T(\boldsymbol{x})$ of the estimator for that particular realization as \vocab{estimate for $\theta$}.
\\
We will study \textbf{3 methods for constructing estimator:} method of moments, maximum likelihood estimation, and Bayes method.
\subsection{Methods of Moments}
Let $X_1,...,X_n$ be a sample from a population with pdf or pmf $f(x|\theta_1,...,\theta_k)$. \vocab{Method of moment estimators} are found by equating the first $k$ sample moments to corresponding $k$ population moments.
\begin{align*}
    m_1 & = \frac{1}{n}\sum_{i=1}^{n}X_i^1, \quad \mu_1(\theta) = \EE X^1  = \int_{-\infty}^{\infty}xf(x|\theta)dx\\
    m_2 & = \frac{1}{n}\sum_{i=1}^{n}X_i^2, \quad \mu_2(\theta) = \EE X^2= \int_{-\infty}^{\infty}x^2f(x|\theta)dx \\
    \vdots \\ 
    m_k & = \frac{1}{n}\sum_{i=1}^{n}X_i^k, \quad \mu_k(\theta) = \EE X^k = 
    \int_{-\infty}^{\infty}x^kf(x|\theta)dx
\end{align*}
The method of moment estimator $(\hat{\theta}_1,..,\hat{\theta}_k)$ of $(\theta_1,...,\theta_k)$ is the solution of the full system of $k$ equations:
\begin{align*}
    \frac{1}{n}\sum_{i=1}^{n}X_i^1 &= \mu_1(\theta) \\
    \frac{1}{n}\sum_{i=1}^{n}X_i^2 &= \mu_2(\theta) \\
    \vdots \\ 
    \frac{1}{n}\sum_{i=1}^{n}X_i^k &= \mu_k(\theta) 
\end{align*}
\textbf{Remark:} $\sum_{i=1}^{n} (X_i-\bar{X})^2 = \sum_{i=1}^{n}X_1^2 + \mu \bar{X}^2 - 2\bar{X}\sum_{i=1}^{n}X_i = \sum_{i=1}^{n} X_i^2 - n\bar{X}^2$
    $$ \implies \sum_{i=1}^{n}X_i^2 = \sum_{i=1}^{n} (X_i-\bar{X})^2 +n\bar{X}^2$$
\subsection{Maximum Likelihood Estimators}
The method of maximum likelihood is by far the most popular technique for deriving estimators. For each observed sample $\boldsymbol{x} = (x_1,...,x_n)$, we define the \vocab{maximum likelihood estimate} as the point $\hat{\theta} = \hat{\theta} (\boldsymbol{x})$ where the function $\theta \mapsto L(\theta | \boldsymbol{x})$ attains its maximum, i,e.:
$$
\underset{\theta \in \Theta}{max}L(\theta|\boldsymbol{x}) = L(\hat{\theta}|\boldsymbol{x})$$
For each sample point $\boldsymbol{x}$, and $ \hat{\theta} (\boldsymbol{x})$ is the parameter value at which  $(\hat{\theta}|\boldsymbol{x})$ attains its maximum as a function of $\theta$, with $\boldsymbol{x}$ held fixed. A \textit{maximum likelihood estimator (MLE)} of the parameter $\theta$ based on sample $\boldsymbol{X}$ is $\hat{\theta}(\boldsymbol{X})$. 
\\
\\
To find the MLE, in most cases we will be solving the equation
$$
\frac{d}{d\theta} L(\theta|\boldsymbol{x}) = 0
$$
and checking that
$$
\frac{d^2}{d\theta^2} L(\theta|\boldsymbol{x}) \mid_{\theta = \hat{\theta}} < 0
$$
