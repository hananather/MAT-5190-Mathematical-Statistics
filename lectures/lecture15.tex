\section{November 10, 2021}
\subsection{Convergence Concepts}
The notion of letting sample size approach infinity can provide us with useful approximations, since it usually happens that expressions become simplified in the limit. 
\\
\subsection{Convergence in Probability}
\begin{definition}[Convergence in probability]
    A sequence of random variables, $X_1,X_2,...$ converges in probability to a random variable $X$ if, for every $\eps > 0$,
    $$
    \lim_{n \to \infty}\PP(|X_n -X| \geq \eps) = 0  \quad \text{or equivalently,} \quad  \lim_{n \to \infty}\PP(|X_n -X| < \eps) = 1
    $$
\end{definition}
The law of large number asserts that as $n$ grows, the sample mean $\bar{X}_n$ converges to true mean $\mu$. Law of larger number has two versions (weak and strong), the difference in the two lies in what is mean for a sequence of random variables to converge to a number. 

\begin{theorem}[Weak Law of Large numbers]
    Let $X_1,X_2,...$ be iid random variables with $\EE X_i = \mu$ and $\VV ar X_i = \sigma^2 < \infty$. Define, $\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i$. Then, for every $\eps > 0$,
    $$
    \lim_{n \to \infty} \PP(|\bar{X}_n - \mu| < \eps ) = 1,
    $$
    that is, $\bar{X}_n$ converges in probability to $\mu$.
\end{theorem}
We can extend the definition of convergence in probability to functions of random variables. Suppose that $X_1,X_2,...$ converges in probability to a random variable $X$ or to a constant $a$ and $h$ is a continuous function. Then $h(X_1), h(X_2),...$ converges in probability to $h(X)$.
\\
We can use the above fact to easily prove that since $S_n^2 \to \sigma^2 \implies S_n = \sqrt{S_n} \to \sigma$.
\begin{note}
    \vocab{Properties of convergence in probability:}
    \begin{enumerate}
        \item if $X_n \xrightarrow{p} X$ and $X_n \xrightarrow{p} Y$, then $X =Y$ asymptotically. 
        \item if $X_n \xrightarrow{p} X$ and if $Y_n \xrightarrow{p} Y$, then if $X_n +Y_n \xrightarrow{p} X + Y$.
        \item If $X_n \xrightarrow{p} X$, $X_n \xrightarrow{p} X$, and $g(x,y)$ is a continuous function, then $g(X_n,Y_n) \xrightarrow{p} g(X,Y)$. 
    \end{enumerate}
\end{note}
\subsection{Almost Sure Convergence and Strong Law of Large Numbers}
Almost sure convergence is stronger than convergence in probability. It is similar to point wise convergence of a sequence of functions.
\begin{definition}[Almost surely convergence]
    A sequence of random variables, $X_1,X_2,...$ converges almost surely to a random variable $X$ if, for every $\eps > 0$,
    $$
    \PP(\lim_{n \to \infty}|X_n - X|< \eps ) = 1.
    $$
\end{definition}
Lets try to understand this definition a bit deeper. A random-variable is just a real-valued function defined on a sample space $S$. Let $s \in S$, then $X_{n}(s)$ and $X(s)$ are all functions defined on $S$. The definition is saying that $X_n$ converges to $X$ almost surely if the functions $X_{n}(s)$ converges to $X(s)$.\\
\\
\term{Note:} \textit{Almost surely convergence} implies convergence in probability, however converse is not true. We say that the sequence of $\{ \hat{\theta}_n \}_n$ is a \textit{strongly consistent} estimators for the parameter $\theta$ if $\hat{\theta}_n \xrightarrow{a.s} \theta$.
\begin{theorem}[The Strong Law of Large Numbers]
    Let $\{ \bar{X}_n \}_n$ be a sequence of iid random variables. Suppose that $\EE|X_1| \leq \infty$ and $\EE X_1 = \mu$. Then 
    $$
    \bar{X}_n \xrightarrow{a.s} \mu 
    $$
    In other words, sample mean $\bar{X}_n$ converges to the true mean $\mu$ pointwise as $n \to \infty$.
\end{theorem}
The law of large number plays a crucial role in simulations and statistics. Lets say we generate data from a large number of i.i.d samples of an experiment, either using a computer simulation or relation world. If we employ proportion of times an event occurred to approximate the probability of the event, we are \textit{implicitly} applying the Law of Large Numbers.
\subsection{Convergence in Distribution and Central Limit Theorem}
\begin{definition} [Convergence in Distribution]
    We say that $\{ X_n \}_{n \geq 1}  = \{ X_1,X_2,... \}$ of random variables \term{converges in distribution} to a random variable $X$ if 
    $$
    F_{X_n}(x) = \PP(X_n \leq x) \xrightarrow{n \to \infty} \PP(X \leq x) = F_{X}(x) \quad \text{for all } x \in \RR \
    $$
    such that $F_X$ is continuous.\\
    We say that sequence $\hat{\theta}_{n \geq 1}$ of estimators of $\theta$ is \vocab{asymptotically normal} for $\theta$ if 
    $$
    \sqrt{n}(\theta_n - \theta) \xrightarrow{d} Z \sim N(0,\sigma^2) \quad \text{ for some } \sigma^2 >0.
    $$
\end{definition}
Lets try to connect the ideas from previous sections to above definition. The law of large numbers essentially says that if we have $X_1,X_2,X_2,...$ i.i.d with mean $\mu$ and variance $\sigma^2$, $\bar{X}_n \to \mu$ as $n \to \infty$ with probability $1$. But what is the distribution of $\bar{X}_n$ along the way to becoming a constant? This is where the Central Limit Theorem (CLT) comes into play. 
\begin{theorem}[Central Limit Theorem]
    Let $\{ X_n \}_{n \geq 1}  = \{ X_1,X_2,... \}$ of random variables, assume that $\EE X_i = \mu$ is finite and $\VV ar(X_i) = \sigma^2 < \infty$.
    Then 
    $$
    \sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} Z \sim N(0,\sigma^2)
    $$
\end{theorem}

If $X_n \xrightarrow{d} X$ and $X_n \xrightarrow{d} a$, then
    $$
    X_n + Y_n \xrightarrow{d} X + a \quad \text{and} \quad
    X_nY_n \xrightarrow{d} Xa
    $$

We say that $\{ X_n \}_{n \geq 1}$ is a sequence of asymptotically normal estimators of $\mu$.
\begin{note}
    \vocab{Delta Method.} Let $\{ \hat{\theta}_n \}_n$ be a sequence of asymptotically normal estimators of $\theta$. Recall that asymptotically normal means that random variables satisfies $\sqrt{n}(Y_n - \theta) \to n(9,\sigma)$ in distribution. For a given function $g$ and specific value of $\theta$, suppose that $g'(\theta)$ exists and is not $0$. Then 
    $$
    \sqrt{n}[g(Y_n)-g(\theta)] \to n(0,\sigma^2[g'(\theta)]^2) \quad \text{in distribution.} 
    $$ 
    
\end{note}