\section{November 3, 2021}
\subsection{Sampling from Normal Distribution}
\begin{proposition}
    Let $X_1,...,X_n$ be a random sample from $N(\mu,\sigma^2)$ population, $\bar{X}$ be the sample mean and $S^2$ be the sample variance. Then, 
    \begin{enumerate}
        \item $\bar{X}$ and $S^2$ are \textbf{independent} random variables
        \item $\bar{X} \sim$ Normal($\mu,\sigma^2/n)$
        \item $(n-1)S^2/\sigma^2 \sim \chi_{n-1}^2$
    \end{enumerate}
\end{proposition}
Recall that if a random vector $(X,Y)$ has a normal distribution, then
$$
X,Y \quad \text{are independent } \iff \text{Cov}(X,Y) = 0
$$
We can actually generalize this idea to linear combinations of normal random variables. Let $X_1,...,X_n$ be random variables such that $X_i \sim N(\mu_i, \sigma_i^2)$ for each $i=1,..n$. Define the random vector $\boldsymbol{U} =(U_1,...,U_k)$ and $\boldsymbol{V} = (V_1,...,V_r)$ where
$$
U_i = \sum_{j=1}^n a_{ij}X_j, \quad i=1,...,k; \quad V_r = \sum_{j=1}^{n}b_{rj}X_j, \quad r=1,...,m
$$
and $ a_{ij}$, $b_{rj}$ are constants. Then,
$$
U_i \sim N \left( \sum_{j=1}^n a_{ij} \mu_i, \sum_{j=1}^n a_{ij}^2 \sigma_i^2 \right),
\quad
V_r \sim N \left( \sum_{j=1}^n b_{rj} \mu_i, \sum_{j=1}^n b_{rj}^2 \sigma_i^2 \right),
\quad
\text{Cov}(U_i,V_r) = \sum_{j=1}^n a_{ij} b_{rj} \sigma_j^2
$$
and, 
$$
U_i,V_r \text{ are independent } \iff \text{Cov}(U_i,V_r) = 0
$$
And this also implies that
$$
\boldsymbol{U},\boldsymbol{V} \text{ are independent random vectors } \iff U_i, V_r \text{ are independent } \forall i, \forall r.
$$
\vocab{Application:} If $X_1,...,X_n$ is a random sample from $N(\mu, \sigma^2)$ population, 
$$
X_j - \bar{X} \text{ and } \bar{X} \text{ are independent}
$$
for every $j=1,...,n$. From here we conclude that $S^2$ and $\bar{X}$ are independent. 

\subsection{The Derived Distributions: Student’s $t$ and Snedecor’s $F$}
\begin{definition}[Student's $t$ distribution]
We say that a random variable $T$ has \vocab{Student's t distribution} with $p$ degrees of freedom (and we write $T \sim t_p$) if its pdf is given by
$$
f(t) = \frac{\Gamma(p+1)/2}{\Gamma(p/2)} \cdot
\frac{1}{(1+t^2/p)^(p+1)/2}, \quad -\infty < t < \infty.
$$
\end{definition}
\textbf{Properties of the $t$ distribution}
\begin{enumerate}
    \item $t_1$ = Cauchy($0,1$)
    \item the graph of student's $t$ distribution is bell-shaped and symmetric around 0.
    \item We have
    $$
    \EE T = 0, \quad \VV ar T = \frac{p}{p-2} \text{ if } p >2.
    $$
\end{enumerate}
Let $U$ and $V$ be random variables such that $U \sim $ Normal(0,1) and $V \sim \chi_p^2$. Then
$$
T := \frac{U}{\sqrt{V/p}} \sim t_p.
$$
\vocab{Application.}