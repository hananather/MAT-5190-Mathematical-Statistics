\section{November 3, 2021}
\subsection{Sampling from Normal Distribution}
\begin{proposition}
    Let $X_1,...,X_n$ be a random sample from $N(\mu,\sigma^2)$ population, $\bar{X}$ be the sample mean and $S^2$ be the sample variance. Then, 
    \begin{enumerate}
        \item $\bar{X}$ and $S^2$ are \textbf{independent} random variables
        \item $\bar{X} \sim$ Normal($\mu,\sigma^2/n)$
        \item $(n-1)S^2/\sigma^2 \sim \chi_{n-1}^2$
    \end{enumerate}
\end{proposition}
Recall that if a random vector $(X,Y)$ has a normal distribution, then
$$
X,Y \quad \text{are independent } \iff \text{Cov}(X,Y) = 0
$$
We can actually generalize this idea to linear combinations of normal random variables. Let $X_1,...,X_n$ be random variables such that $X_i \sim N(\mu_i, \sigma_i^2)$ for each $i=1,..n$. Define the random vector $\boldsymbol{U} =(U_1,...,U_k)$ and $\boldsymbol{V} = (V_1,...,V_r)$ where
$$
U_i = \sum_{j=1}^n a_{ij}X_j, \quad i=1,...,k; \quad V_r = \sum_{j=1}^{n}b_{rj}X_j, \quad r=1,...,m
$$
and $ a_{ij}$, $b_{rj}$ are constants. Then,
$$
U_i \sim N \left( \sum_{j=1}^n a_{ij} \mu_i, \sum_{j=1}^n a_{ij}^2 \sigma_i^2 \right),
\quad
V_r \sim N \left( \sum_{j=1}^n b_{rj} \mu_i, \sum_{j=1}^n b_{rj}^2 \sigma_i^2 \right),
\quad
\text{Cov}(U_i,V_r) = \sum_{j=1}^n a_{ij} b_{rj} \sigma_j^2
$$
and, 
$$
U_i,V_r \text{ are independent } \iff \text{Cov}(U_i,V_r) = 0
$$
And this also implies that
$$
\boldsymbol{U},\boldsymbol{V} \text{ are independent random vectors } \iff U_i, V_r \text{ are independent } \forall i, \forall r.
$$
\vocab{Application:} If $X_1,...,X_n$ is a random sample from $N(\mu, \sigma^2)$ population, 
$$
X_j - \bar{X} \text{ and } \bar{X} \text{ are independent}
$$
for every $j=1,...,n$. From here we conclude that $S^2$ and $\bar{X}$ are independent. 

\subsection{The Derived Distributions: Student’s $t$ and Snedecor’s $F$}
\begin{definition}[Student's $t$ distribution]
We say that a random variable $T$ has \vocab{Student's t distribution} with $p$ degrees of freedom (and we write $T \sim t_p$) if its pdf is given by
$$
f(t) = \frac{\Gamma(p+1)/2}{\Gamma(p/2)} \cdot
\frac{1}{(1+t^2/p)^(p+1)/2}, \quad -\infty < t < \infty.
$$
\end{definition}
\textbf{Properties of the $t$ distribution}
\begin{enumerate}
    \item $t_1$ = Cauchy($0,1$)
    \item the graph of student's $t$ distribution is bell-shaped and symmetric around 0.
    \item We have
    $
    \EE T = 0, \quad \VV ar T = \frac{p}{p-2} \text{ if } p >2.
    $
\end{enumerate}
Let $U$ and $V$ be random variables such that $U \sim $ Normal(0,1) and $V \sim \chi_p^2$. Then
$$
T := \frac{U}{\sqrt{V/p}} \sim t_p.
$$
\vocab{Application.} Let $X_1,...,X_n$ be a random sample from Normal($\mu$, $\sigma^2$) population. Then
$$
T := \frac{\bar{X} - \mu}{S / \sqrt{n}} \sim t_{n-1}.
$$
\textbf{T is used to make inferences about $\mu$, when $\sigma^2$ is unknown.}
\begin{definition}[Snedecor’s F distribution]
We say that a continuous random variable $F$ has a Snedecor’s F distribution with $p$ and $q$ degrees of freedom (and we write $F \sim F_{p,q}$) if its pdf is given by
$$
f(x) = \frac{\Gamma((p+q)/2)}{\Gamma(p/2)\Gamma(q/2)} \cdot \left( 
\frac{p}{q}
\right)^{p/2} \cdot
\frac{x^{(p/2)-1}}{[1+(p/q)x]^{(p+q)/2)}}, \quad 0 < x < \infty.
$$
\end{definition}
Let $U$ and $V$ be independent random variables such that $U \sim \chi_{p}^2$ and $V \sim \chi_{q}^2$. Then
$$
F := \frac{U/p}{V/p} \sim F_{p,q}.
$$
\vocab{Application.} Let $\boldsymbol{X} = (X_1,...,X_n)$ be a random sample from Normal($\mu_X, \sigma_X^2$) and $\boldsymbol{Y} = (Y_1,...,Y_n)$ be a random sample from a Normal($\mu_Y,\sigma_{Y}^2$). Suppose that $\boldsymbol{X}$ and $\boldsymbol{Y}$ are independent. Then,
$$
F := \frac{S_{X}^2/ \sigma^2}{S_{Y}^2/ \sigma^2_Y} \sim F_{m-1,n-1}
$$
$F$ is used to make inferences about the ratio of $\sigma_{X}^2 / \sigma_{Y}^2 $
