\section{November 17, 2021}
\subsection{Factorization Theorem Continued}
\begin{example}
    Let $X_1,...,X_n$ be independent random variables such that $$X_k \sim Unif(k(\theta -1), k(\theta+1)).$$
    
    Show that $ \left ( \displaystyle \underset{1 \leq k \leq n}{\mathrm{min}} \frac{x_k}{k}, \underset{1 \leq k \leq n}{\mathrm{max}} \frac{x_k}{k}
    \right )$ is a $2-$dimensional sufficient statistic. 
    
\end{example}

\begin{theorem}
    Let $\boldsymbol{X} = (X_1,...,X_n)$ be a random sample from a pdf(or pmf) which belongs to the following exponential family
    $$
    f_X(x|\theta) = h(x)c(\theta)exp 
    \left \{ 
    \sum_{i=1}^{k}w_i(\theta)t_i(x)
    \right \}
    $$
    with $\theta = (\theta_1,...,\theta_d)$. Suppose that $d \leq k$. Define 
    $$T_i = T_i(\boldsymbol{X}) = \sum_{j=1}^{n}T_i(X_j)
    \quad \forall i=1,...,k.$$
    Then $\boldsymbol{T} = (T_1,...,T_k)$ is a sufficient statistic for $\theta$.
\end{theorem}
\begin{proof}
    The joint pdf of $\boldsymbol{X}$ is:
    \begin{align*}
    f(\boldsymbol{x}|\theta) & = 
    \prod_{j=1}^{n}f_{x_j}(x_j \theta) 
    = \prod_{j=1}^{n} \left[
    h(x_j)c(\theta) \left \{
    \sum_{i=1}^{k}w_i(\theta)t_i(x_j)
    \right\}
    \right ] \\
    & = \prod_{j=1}^{n}h(x_j)[c(\theta)]^n \mathrm{exp} \left \{
    \sum_{i=1}^{k}w_i(\theta)t_i(x_j)
    \right\}
    \end{align*}
    Clearly, 
    $$
    \underbrace{ \prod_{j=1}^{n}h(x_j)}_{h'(x)}
    \underbrace{[c(\theta)]^n\mathrm{exp} \left \{
    \sum_{i=1}^{k}w_i(\theta)t_i(x_j)
    \right\}}_{g(T_1(\boldsymbol{x}),...,T_k(\boldsymbol{x})|\theta)}
    $$
    Thus, by the Factorization theorem $(T_1(\boldsymbol{x}),...,T_k(\boldsymbol{x})|\theta)$ is a sufficient statistic for $\theta$.
\end{proof}
\term{Remark:} If $\boldsymbol{T}$ is a sufficient statistic for $\theta$, then any one-to-one transformation of $\boldsymbol{S} = \pi(\boldsymbol{T})$ is also a sufficient statistic for for $\theta$, $\forall \pi$ one-to-one maps. \textbf{Therefore, a sufficient statistic is not unique}. 
\begin{definition}
    A sufficient statistic is $\boldsymbol{T} = (T_1,...,T_k)$ is called a \vocab{minimal sufficient statistic} for $\theta$ if, for any other sufficient statistic $\boldsymbol{T}^* = (T_1^*,...,T_k^*)$, there exists a function $\phi$ such that $\boldsymbol{T} = \phi(\boldsymbol{T}^*)$. This is equivalent to saying that
    $$
    T^*(\boldsymbol{x}) = T^*(\boldsymbol{y}) \Longrightarrow T(\boldsymbol{x}) = T(\boldsymbol{y})
    $$
\end{definition}
\term{Note:} A minimal sufficient statistic may \textbf{NOT} be unique.
\begin{theorem}[Lehman and Scheffe]
    Let $\boldsymbol{X} = (X_1,...,X_n)$ be a random sample from a pdf(or pmf) which belongs to the following exponential family $f_X(x|\theta)$. Let $T = T(\boldsymbol{X})$ be a statistic which satisfies the following condition
    $$
    \frac{f(\boldsymbol{x}|\theta)}{f(\boldsymbol{y}|\theta)}\quad \textbf{does not depend on } \theta
    \iff T(\boldsymbol{x}) =  T(\boldsymbol{y})
    $$
    Then $T$ is a minimal sufficient statistic. 
\end{theorem}
\begin{proof}
    Proof is given in the text book.
\end{proof}