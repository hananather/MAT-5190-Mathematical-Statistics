\section{December 1, 2021}
\subsection{Methods of evaluating estimators}
The general topic of evaluating statistical procedures is part of a branch of statistics called decision theory. In this section we look at some basic criteria for evaluating estimators.
    
\begin{definition}[Mean square error (MSE)]
    The \vocab{mean square error} of an estimator $W$ of a parameter $\theta$ is the function of $\theta$ defined by $\EE_{\theta}(W-\theta)^2$.
\end{definition}

\begin{definition}[Bias]
    The bias of an estimator $T$ for a parameter $\theta$ is defined by 
    $$
    \text{Bias}_{\theta}(T) = \EE_{\theta}T -\theta
    $$
\end{definition}
\textbf{Note:} $MSE$ incorporates two components, one measuring the variability and the other measuring bias.
$$
\text{MSE}_{\theta} = \VV ar_{\theta}(T) + (\text{Bias}_{\theta}(T))^2
$$
\begin{example}
    Let $X-1,...,X_n$ be iid $n(\mu,\sigma^2)$. The statistics $\bar{X}$ and $\S^2$ are both biased estimators since
    $$
    \EE \bar{X} = \mu \quad, \EE S^2 = \sigma^2, \quad \text{for all $\mu$ and $\sigma$}.
    $$
    The MSEs of the estimators are given by
    $$
    \EE(\bar{X} - \mu)^2 = \VV ar \bar{X} = \frac{\sigma}{n}.
    $$
    $$
    \EE(S^2 \sigma^2)^2 = \VV ar S^2
    $$
\end{example}
\subsection{Best unbiased estimators (UMVUE)}
If we compare estimators based on MSE, there is no single \textit{"best MSE"} estimator. The reason is that the class of all estimators is too large of a class. Therefore by placing certian restrictions on our estimators, we can limit the class of estimators.  
\begin{definition}[Best unbiased estimator]
    An estimator $W^*$ is the \vocab{best unbiased estimator} of $\tau(\theta)$ if it satisfies $\EE_{\theta}W^* = \tau(\theta)$ for all $\theta$ and, for any for any other estimator $W$ with $\EE_{\theta}W = \tau(\theta)$, we have $\VV ar_{\theta}W^* \leq \VV ar_{\theta} W$ for all $\theta$. $W^*$ is also called a \vocab{uniform minimum variance unbiased estimator (UMVUE)}
\end{definition}