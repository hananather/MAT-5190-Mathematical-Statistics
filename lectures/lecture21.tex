\section{December 1, 2021}
\subsection{Methods of evaluating estimators}
The general topic of evaluating statistical procedures is part of a branch of statistics called decision theory. In this section we look at some basic criteria for evaluating estimators.
    
\begin{definition}[Mean square error (MSE)]
    The \vocab{mean square error} of an estimator $W$ of a parameter $\theta$ is the function of $\theta$ defined by $\EE_{\theta}(W-\theta)^2$.
\end{definition}

\begin{definition}[Bias]
    The bias of an estimator $T$ for a parameter $\theta$ is defined by 
    $$
    \text{Bias}_{\theta}(T) = \EE_{\theta}T -\theta
    $$
\end{definition}
\textbf{Note:} $MSE$ incorporates two components, one measuring the variability and the other measuring bias.
$$
\text{MSE}_{\theta} = \VV ar_{\theta}(T) + (\text{Bias}_{\theta}(T))^2
$$
\begin{example}
    Let $X-1,...,X_n$ be iid $n(\mu,\sigma^2)$. The statistics $\bar{X}$ and $\S^2$ are both biased estimators since
    $$
    \EE \bar{X} = \mu \quad, \EE S^2 = \sigma^2, \quad \text{for all $\mu$ and $\sigma$}.
    $$
    The MSEs of the estimators are given by
    $$
    \EE(\bar{X} - \mu)^2 = \VV ar \bar{X} = \frac{\sigma}{n}.
    $$
    $$
    \EE(S^2 \sigma^2)^2 = \VV ar S^2
    $$
\end{example}
\subsection{Best unbiased estimators (UMVUE)}
If we compare estimators based on MSE, there is no single \textit{"best MSE"} estimator. The reason is that the class of all estimators is too large of a class. Therefore by placing certian restrictions on our estimators, we can limit the class of estimators.  
\begin{definition}[Best unbiased estimator]
    An estimator $W^*$ is the \vocab{best unbiased estimator} of $\tau(\theta)$ if it satisfies $\EE_{\theta}W^* = \tau(\theta)$ for all $\theta$ and, for any for any other estimator $W$ with $\EE_{\theta}W = \tau(\theta)$, we have $\VV ar_{\theta}W^* \leq \VV ar_{\theta} W$ for all $\theta$. $W^*$ is also called a \vocab{uniform minimum variance unbiased estimator (UMVUE)}
\end{definition}
\begin{theorem}[Cramer-Rao Lower Bound]
    Let $X_1,...,X_n$ be a random variables with joint pdf $f(\boldsymbol{x}|\theta)$. Let $W = W(\boldsymbol{X})$ be a finite variance estimator satisfying the following conditio:
    $$
    \frac{d}{d\theta} \int W(\boldsymbol{x})f(\boldsymbol{x}|\theta)d\boldsymbol{x} = 
    \int W(\boldsymbol{x}) \frac{d}{d \theta}f(\boldsymbol{x}|\theta)d \boldsymbol{x}.
    $$
    Then 
    $$
    \VV ar_{\theta}W \geq \frac{\left( \cfrac{d}{d\theta} \EE_\theta W \right)^2}{I_n(\theta)}
    = \text{ CR-Lower-Bound}
    $$
    Where
    $$
    I_n(\theta) = \EE_{\theta}\left (
    \frac{d}{d \theta} log f(\boldsymbol{X}|\theta)
    \right)^2 = \textbf{Fisher information}
    $$
\end{theorem}
\vocab{Note:}
\begin{itemize}
    \item If $W$ is an unbiased estimator of $\theta$ then $\frac{d}{d \theta} \EE_{\theta}W =1$, and thus the CR-Lower-Bound becomes $\cfrac{1}{I_n(\theta)}$.
\end{itemize}