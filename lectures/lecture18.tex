\section{November 22, 2021}
\subsection{The Sufficiency Principle (Continued)}
So far, we have covered sufficient statistics which in a sense contain all the information about $\theta$ that is available in the sample. Next we look at a statistic which is quite the opposite.
\begin{definition}[Ancillary Statistic]
    A statistic $S(\boldsymbol{X})$ whose distribution does not depend on the parameter
    $\theta$ is called an \vocab{ancillary} statistic.
\end{definition}
An ancillary statistic contains no information about $\theta$! An ancillary statistic has a fixed and known distribution that is unrelated to $\theta.$
\begin{example}[Location and Scale family ancillary statistic]
    Let $X_1,...,X_n$ be iid observations from a location parameter family with pdf $g(x|\mu) = f(x-\mu)$ where $f$ is a standard pdf. Then we will show that
    $$
    \begin{cases}
        R = X_{(n)} - N_{(1)} \quad \text{is ancillary statistic for } \mu. \\
        S^2 = \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \Bar{X})^2 \quad \text{is ancillary statistic for } \mu.
    \end{cases}
    $$ 
    In addition to that if we have random sample $\boldsymbol{X} = (X_1,...,X_n)$ from a \textit{scale family} with pdf $g(x|\theta) = \frac{1}{\sigma}f(x/\sigma)$, where $f(z)$ is the standard pdf of the family. Then
    $$
    T(\boldsymbol{X}) = \left( 
    \frac{X_1}{X_n},...,\frac{X_{n-1}}{X_n} 
    \right)
    \text{ is an ancillary statistic for }\sigma
    $$
    In particular $\bar{X}/X_n$ is an ancillary statistic for $\sigma$.
    \begin{proof}
        
    \end{proof}
\end{example}
\begin{definition}[Complete statistic]
    Let $f(t|\theta)$ be a family of pdf or omfs for a statistic $T(\boldsymbol{X})$. The family of probability distributions is called \vocab{complete} if $\EE_{\theta}(T) = 0$ for all $\theta$ implies that $\PP_{\theta}(g(T) =0) =1$ for all $\theta$. Equivalently, $T(\boldsymbol{X})$ is called a \vocab{complete statistic}.
    \\
    In other words, let $T$ be the statistic whose range is $\ST$, it is called complete if
    $$
    \EE g(T) = 0 \text{ for all } \theta \implies g(t) =0 \text{ for all } t \in \ST
    $$
\end{definition}
\begin{theorem}[Basu's Theorem]
    If $T(\boldsymbol{X})$ is a complete ad minimal sufficient statistic, the $T(\boldsymbol{X})$ is independent of every ancillary statistic. 
\end{theorem}
\begin{proof}
    The proof for the discrete case is given on page 287 of the textbook, review it!
\end{proof}
\begin{theorem}[Complete Statistics in the exponential families]
    Let $\boldsymbol{X} = (X_1,...,X_n)$ be iid observations from an exponential family with pdf or pmf of the form
    $$
        f(x|\theta) = h(x)c(\theta)\text{exp} \left (
            \sum_{j=1}^{k}w(\theta_j)t_j(x),
        \right) 
    $$
    where $\boldsymbol{\theta} = (\theta_1,...,\theta_k)$. Then the statistic 
    $$
    T(\boldsymbol{X}) = \left (
        \sum_{i=1}^{n}t_1(X_i), \sum_{i=1}^{n}t_2(X_i),...,\sum_{i=1}^{n}t_k(X_i),
    \right )
    $$
    is complete as long as the parameter space $\Theta$ contains an open set in $\RR^k$.
\end{theorem}
\begin{proof}
    Proof is omitted from this course.
\end{proof}
\subsection{Likelihood Principle}
\begin{definition}[Likelihood function]
    Let $f(\boldsymbol{x}|\theta)$ denote the joint pdf or pmf of the sample $\boldsymbol{X} = (X_1,...,X_n)$. Then, given that $\boldsymbol{X} = \boldsymbol{x}$ is observed, the function of $\theta$ defined by $$
    L(\theta|\boldsymbol{x}) = f(\boldsymbol{x}|\theta)
    $$
    is called the \vocab{likelihood function}.
\end{definition}
\textbf{Likelihood Principle:} If $\boldsymbol{x}$ and $\boldsymbol{y}$ are two sample points such that $L(\theta|\boldsymbol{x}) \propto L(\theta|\boldsymbol{y})$, i.e, $\exists C(\boldsymbol{x},\boldsymbol{y})$ constant such that
$$
    L(\theta|\boldsymbol{x}) = C(\boldsymbol{x},\boldsymbol{y}) L(\theta|\boldsymbol{y}) \quad \text{for all } \theta,
$$
then the conclusions drawn from $\boldsymbol{x}$ and $\boldsymbol{y}$ should be identical. 
We define an \textbf{experiment} $E$ to be a triple $(\boldsymbol{X}, \theta, \{f(\boldsymbol{x}|\theta)\})$, wher $\boldsymbol{X}$ is a random vector with pmf $f(\boldsymbol{x}|\theta)$ for some $\theta \in \Theta$. An experimenter knowing what experiment $E$ was performed hand having observed a particular $\boldsymbol{X} = \boldsymbol{x}$, will make some inference or draw soem conclusion about $\theta$. We denote this conclusion by Ev$(E,\boldsymbol{x})$, which stands for \textit{evidence about $\theta$ arising from $E$ and $\boldsymbol{x}$}.
\\
\textbf{Formal sufficiency Principle:} Consider experiment $E = (\boldsymbol{X}, \theta,\{f(\boldsymbol{x}|\theta)\})$ and suppose that is $T(\boldsymbol{X})$ a sufficient statistic for $\theta$. If $\boldsymbol{x}$ and $\boldsymbol{y}$ are two samples such that $T(\boldsymbol{x}) = T(\boldsymbol{y})$ then
$$
\text{Ev}(E,\boldsymbol{x}) = \text{Ev}(E,\boldsymbol{y})
$$
\textbf{Formal Likelihood Principle:} Suppose that we have two experiments, $E_1 = (\boldsymbol{X}_1, \theta, \{f(\boldsymbol{x}_1|\theta)\})$ and $E_2 = (\boldsymbol{X}_2, \theta, \{f(\boldsymbol{x}_2|\theta)\})$, where we have the unknown parameter $\theta$ is the same for both experiments. If $\boldsymbol{x}$ is a sample from $E_1$ and $\boldsymbol{y}$ is a sample from $E_2$ such that 
$$
    L(\theta|\boldsymbol{x}) = C(\boldsymbol{x},\boldsymbol{y}) L(\theta|\boldsymbol{y}) \quad \text{for all } \theta,
$$
then,
$$
\text{Ev}(E_1,\boldsymbol{x}) = \text{Ev}(E_2,\boldsymbol{y})
$$